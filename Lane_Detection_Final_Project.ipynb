{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgipWVe1BpmV"
   },
   "source": [
    "# **Lane Detection**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEVhhd55D16K"
   },
   "source": [
    "TEAM MEMBERS:\n",
    "1.  Ujwal Kothapally\n",
    "2.  Venkata Krishna Sreekar Padakandla\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBUuTnnzVQJ2"
   },
   "source": [
    "# Install Roboflow Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uIXoY-6uHJbd",
    "outputId": "818cbd2e-b3e2-4aab-a189-77e2d36f5fe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: roboflow in c:\\users\\91767\\lokesh\\lib\\site-packages (1.1.9)\n",
      "Requirement already satisfied: certifi==2023.7.22 in c:\\users\\91767\\lokesh\\lib\\site-packages (from roboflow) (2023.7.22)\n",
      "Requirement already satisfied: chardet==4.0.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from roboflow) (4.0.0)\n",
      "Requirement already satisfied: cycler==0.10.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from roboflow) (0.10.0)\n",
      "Requirement already satisfied: idna==2.10 in c:\\users\\91767\\lokesh\\lib\\site-packages (from roboflow) (2.10)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\91767\\lokesh\\lib\\site-packages (from roboflow) (1.4.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\91767\\lokesh\\lib\\site-packages (from roboflow) (3.7.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\91767\\lokesh\\lib\\site-packages (from roboflow) (1.24.3)\n",
      "Requirement already satisfied: opencv-python-headless==4.8.0.74 in c:\\users\\91767\\lokesh\\lib\\site-packages (from roboflow) (4.8.0.74)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in c:\\users\\91767\\lokesh\\lib\\site-packages (from roboflow) (9.4.0)\n",
      "Requirement already satisfied: pyparsing==2.4.7 in c:\\users\\91767\\lokesh\\lib\\site-packages (from roboflow) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\91767\\lokesh\\lib\\site-packages (from roboflow) (2.8.2)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\91767\\lokesh\\lib\\site-packages (from roboflow) (1.0.0)\n",
      "Requirement already satisfied: requests in c:\\users\\91767\\lokesh\\lib\\site-packages (from roboflow) (2.31.0)\n",
      "Requirement already satisfied: six in c:\\users\\91767\\lokesh\\lib\\site-packages (from roboflow) (1.16.0)\n",
      "Requirement already satisfied: supervision in c:\\users\\91767\\lokesh\\lib\\site-packages (from roboflow) (0.16.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in c:\\users\\91767\\lokesh\\lib\\site-packages (from roboflow) (1.26.16)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from roboflow) (4.65.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in c:\\users\\91767\\lokesh\\lib\\site-packages (from roboflow) (6.0)\n",
      "Requirement already satisfied: requests-toolbelt in c:\\users\\91767\\lokesh\\lib\\site-packages (from roboflow) (1.0.0)\n",
      "Requirement already satisfied: python-magic in c:\\users\\91767\\lokesh\\lib\\site-packages (from roboflow) (0.4.27)\n",
      "Requirement already satisfied: colorama in c:\\users\\91767\\lokesh\\lib\\site-packages (from tqdm>=4.41.0->roboflow) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\91767\\lokesh\\lib\\site-packages (from matplotlib->roboflow) (1.0.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from matplotlib->roboflow) (4.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from matplotlib->roboflow) (23.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\91767\\lokesh\\lib\\site-packages (from requests->roboflow) (2.0.4)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.9.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from supervision->roboflow) (1.10.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install roboflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scw3Tg_T8yx9"
   },
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "X4vQM4rZ8oYo"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xitiONT9BfP"
   },
   "source": [
    "# Install PyTorch and torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X53je83i88wv",
    "outputId": "b01f5fdc-7bce-4b47-85ae-686186a00898"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\91767\\lokesh\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\91767\\lokesh\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\91767\\lokesh\\lib\\site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\91767\\lokesh\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\91767\\lokesh\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\91767\\lokesh\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\91767\\lokesh\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\91767\\lokesh\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\91767\\lokesh\\lib\\site-packages (0.16.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\91767\\lokesh\\lib\\site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: requests in c:\\users\\91767\\lokesh\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.1 in c:\\users\\91767\\lokesh\\lib\\site-packages (from torchvision) (2.1.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\91767\\lokesh\\lib\\site-packages (from torch==2.1.1->torchvision) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\91767\\lokesh\\lib\\site-packages (from torch==2.1.1->torchvision) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\91767\\lokesh\\lib\\site-packages (from torch==2.1.1->torchvision) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\91767\\lokesh\\lib\\site-packages (from torch==2.1.1->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\91767\\lokesh\\lib\\site-packages (from torch==2.1.1->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\91767\\lokesh\\lib\\site-packages (from torch==2.1.1->torchvision) (2023.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\91767\\lokesh\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91767\\lokesh\\lib\\site-packages (from requests->torchvision) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\91767\\lokesh\\lib\\site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91767\\lokesh\\lib\\site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from jinja2->torch==2.1.1->torchvision) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\91767\\lokesh\\lib\\site-packages (from sympy->torch==2.1.1->torchvision) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JuxiK7QICd2p",
    "outputId": "f7e969d9-3912-4b41-bdf4-b0791cf2f8b8"
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnjnXx309TDl"
   },
   "source": [
    "# Import PyTorch and Other Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Mh6IeOoO9TNQ"
   },
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19NoDSvQ9uak"
   },
   "source": [
    "# Downloads a dataset from Roboflow using the YOLOv5 model format, specifying the API key, project name and version, and saves it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kgyfTYoHHUvf",
    "outputId": "7b590666-e2db-41e0-a640-4425e4d38586"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in /Downloads to yolov5pytorch:: 100%|██████████| 1885/1885 [00:00<00:00, 10686.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to /Downloads in yolov5pytorch:: 100%|████████████████| 58/58 [00:00<00:00, 1724.90it/s]\n"
     ]
    }
   ],
   "source": [
    "rf = Roboflow(api_key=\"aJ3Qo0kyqheDC6TpzDW0\", model_format=\"yolov5\")\n",
    "dataset = rf.workspace().project(\"lane-detection-xyuzw\").version(1).download(location=\"/Downloads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVsVYsSh-LQh"
   },
   "source": [
    "# Custom PyTorch dataset class (CustomDataset) that wraps around the ImageFolder class, allowing for **customization of the data loading process, sets a seed for reproducibility, defines image transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sEaNsBssHhcC"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.dataset = ImageFolder(root_dir, transform=transform)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset = CustomDataset(root_dir=\"/Downloads\", transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVfme7bu-oP4"
   },
   "source": [
    "# Custom PyTorch neural network model (LaneDetectionModel) based on the DeepLabV3 architecture with a ResNet-101 backbone, loads pre-trained weights, and modifies the output layer to suit the task of lane detection by changing the number of output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0OxhpnxMHhhi",
    "outputId": "b29761eb-b425-4485-cd86-252b5e11c9a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaneDetectionModel(\n",
      "  (deeplabv3): DeepLabV3(\n",
      "    (backbone): IntermediateLayerGetter(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (5): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (6): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (7): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (8): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (9): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (10): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (11): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (12): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (13): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (14): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (15): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (16): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (17): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (18): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (19): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (20): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (21): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (22): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): DeepLabHead(\n",
      "      (0): ASPP(\n",
      "        (convs): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): ASPPConv(\n",
      "            (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): ASPPConv(\n",
      "            (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (3): ASPPConv(\n",
      "            (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (4): ASPPPooling(\n",
      "            (0): AdaptiveAvgPool2d(output_size=1)\n",
      "            (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (project): Sequential(\n",
      "          (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU()\n",
      "      (4): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (aux_classifier): FCNHead(\n",
      "      (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LaneDetectionModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(LaneDetectionModel, self).__init__()\n",
    "\n",
    "        # Load pre-trained DeepLabV3 model\n",
    "        self.deeplabv3 = models.segmentation.deeplabv3_resnet101(pretrained=True)\n",
    "    \n",
    "        # Modify the output layer based on your needs\n",
    "        in_channels = self.deeplabv3.classifier[-1].in_channels\n",
    "        self.deeplabv3.classifier[-1] = nn.Conv2d(in_channels, num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.deeplabv3(x)['out']\n",
    "\n",
    "# Create an instance of the model\n",
    "lane_model = LaneDetectionModel()\n",
    "\n",
    "# Print the model architecture\n",
    "print(lane_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxHlfW-e-siP"
   },
   "source": [
    "# Instantiates an instance of the previously defined LaneDetectionModel with one output class, sets up Mean Squared Error (MSE) loss, defines an Adam optimizer with a learning rate of 0.001, specifies the number of training epochs (5), and splits the dataset into training, validation, and test sets using random splitting. Finally, creates DataLoader instances for the training, validation, and test sets with batch size 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rEDy_LX-HhkW"
   },
   "outputs": [],
   "source": [
    "lane_model = LaneDetectionModel(num_classes=1)\n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# Define optimizer (e.g., Adam)\n",
    "optimizer = optim.Adam(lane_model.parameters(), lr=0.001)\n",
    "num_epochs = 5\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, temp_dataset = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "val_dataset, test_dataset = torch.utils.data.random_split(temp_dataset, [val_size, test_size])\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25t6_pjs_fZ2"
   },
   "source": [
    "# Set of image transforms, including resizing, random horizontal flipping, color jittering, converting to a PyTorch tensor, and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pOGPynVPHhm0"
   },
   "outputs": [],
   "source": [
    "# Define transforms for normalization and augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load dataset with the defined transforms\n",
    "dataset = CustomDataset(root_dir=\"/Downloads\", transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9DJNXQuAG6-"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# A function (visualize_samples) to visualize the first sample of each minibatch from a given DataLoader, creates a DataLoader for visualization with a batch size of 8 and shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "h_XOi-_dHhpj"
   },
   "outputs": [],
   "source": [
    "# Visualize the 1st sample of each minibatch of size 8\n",
    "def visualize_samples(loader):\n",
    "    data_iter = iter(loader)\n",
    "    images, _ = next(data_iter)\n",
    "\n",
    "    for i in range(images.shape[0]):\n",
    "        plt.figure()\n",
    "        plt.imshow(images[i].permute(1, 2, 0))\n",
    "        plt.title(f\"Sample {i+1}\")\n",
    "        plt.show()\n",
    "\n",
    "# Create DataLoader for visualization\n",
    "visualize_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Visualize the 1st sample of each minibatchvisualize_samples(visualize_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HSIVFJeAZqk"
   },
   "source": [
    "# Set the neural network model (lane_model) to training mode, iterates through epochs and mini-batches in the training DataLoader (train_loader), performs a forward pass, calculates the Mean Squared Error (MSE) loss between the model's output and the target, backpropagates the gradients, and updates the model's weights using the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZZkhH8soHhsh",
    "outputId": "e7eb6af6-fe2d-4436-f27a-a3ca16072453"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output Size: torch.Size([8, 1, 224, 224])\n",
      "Epoch 1, Batch 1, Loss: 0.5765178799629211\n",
      "Model Output Size: torch.Size([8, 1, 224, 224])\n",
      "Epoch 2, Batch 1, Loss: 0.6643367409706116\n",
      "Model Output Size: torch.Size([8, 1, 224, 224])\n",
      "Epoch 3, Batch 1, Loss: 0.7168167233467102\n",
      "Model Output Size: torch.Size([8, 1, 224, 224])\n",
      "Epoch 4, Batch 1, Loss: 0.9621191024780273\n",
      "Model Output Size: torch.Size([8, 1, 224, 224])\n",
      "Epoch 5, Batch 1, Loss: 0.5891720652580261\n"
     ]
    }
   ],
   "source": [
    "lane_model.train()  # Set the model to training mode\n",
    "#target = target.view(-1, 1, 1, 1).expand_as(output)\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if batch_idx == 0:  # Only for the first mini-batch\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            output = lane_model(data)  # Forward pass\n",
    "            target = target.view(-1, 1, 1, 1).expand_as(output).float()\n",
    "            print(f\"Model Output Size: {output.shape}\")\n",
    "            loss = criterion(output, target)  # Calculate the loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update the weights\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_idx+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ujgjlgyAiqX"
   },
   "source": [
    "# Transform in the CustomDataset class, setting it to resize images to 32 by 32 pixels, convert them to PyTorch tensors, and normalize the pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "WJge7TSEHhvL"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize to 32 by 32\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Update the transform in CustomDataset\n",
    "dataset = CustomDataset(root_dir=\"/Downloads\", transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEoEijEaApOh"
   },
   "source": [
    "# Update the optimizer with a new learning rate, perform a validation loop to find a suitable learning rate by evaluating the model on the validation set, and then updates the optimizer with the chosen learning rate. Finally, trains the model using the updated optimizer on the training set for the specified number of epochs. The training progress, including epoch number, batch number, and loss, is printed during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aj-QGGa2HhyM",
    "outputId": "08c45967-2432-4af9-ae49-08301886fa9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Learning rate: 0.001, Train Loss: -0.12326176913888452, Validation Loss: -4850584.166666667\n",
      "Epoch 2, Learning rate: 0.0001, Train Loss: -0.9648235413349338, Validation Loss: -5.944232898846773\n",
      "Epoch 3, Learning rate: 1e-05, Train Loss: -0.9976361976910589, Validation Loss: -2.44567851475828\n",
      "Epoch 4, Learning rate: 1.0000000000000002e-06, Train Loss: -0.8335243501448227, Validation Loss: -2.4701258136192337\n",
      "Epoch 5, Learning rate: 1.0000000000000002e-07, Train Loss: -1.0061096877778053, Validation Loss: -2.5742471220049388\n",
      "Epoch 1, Learning rate: 0.010000000000000002, Train Loss: -4.435871084531148, Validation Loss: -21731158086997.332\n",
      "Epoch 2, Learning rate: 0.0010000000000000002, Train Loss: -19.58624683486091, Validation Loss: -15.987458494815902\n",
      "Epoch 3, Learning rate: 0.00010000000000000003, Train Loss: -29.866581701570087, Validation Loss: -55.426045735677086\n",
      "Epoch 4, Learning rate: 1.0000000000000004e-05, Train Loss: -23.929314533869427, Validation Loss: -54.94017028808594\n",
      "Epoch 5, Learning rate: 1.0000000000000004e-06, Train Loss: -25.420297430621186, Validation Loss: -52.26294453938802\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "learning_rates = [0.01, 0.1]\n",
    "best_val_loss = float('inf')  # Initialize with a large value\n",
    "\n",
    "for lr in learning_rates:\n",
    "    optimizer = optim.Adam(lane_model.parameters(), lr=lr)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        lane_model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = lane_model(data)\n",
    "            target = target.view(-1, 1, 1, 1).expand_as(output).float()\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation loop\n",
    "        lane_model.eval()\n",
    "        val_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            with torch.no_grad():\n",
    "                output = lane_model(data)\n",
    "                target = target.view(-1, 1, 1, 1).expand_as(output).float()\n",
    "                loss = criterion(output, target)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}, Learning rate: {optimizer.param_groups[0]['lr']}, \"\n",
    "              f\"Train Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "        # Save the model if it has the best validation loss\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(lane_model.state_dict(), 'best_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5VqL0vPTEiXw",
    "outputId": "b2a0626f-470e-44b1-d0db-de12d69b7ed7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1, Loss: 32.45806121826172\n",
      "Epoch 1, Batch 2, Loss: -70.83707427978516\n",
      "Epoch 1, Batch 3, Loss: -35.88535690307617\n",
      "Epoch 1, Batch 4, Loss: -54.305545806884766\n",
      "Epoch 1, Batch 5, Loss: -41.303653717041016\n",
      "Epoch 1, Batch 6, Loss: -70.14082336425781\n",
      "Epoch 1, Batch 7, Loss: -35.337650299072266\n",
      "Epoch 1, Batch 8, Loss: 2.387404940407123e-09\n",
      "Epoch 1, Batch 9, Loss: -69.35992431640625\n",
      "Epoch 1, Batch 10, Loss: 0.0\n",
      "Epoch 1, Batch 11, Loss: -42.66969680786133\n",
      "Epoch 1, Batch 12, Loss: 0.5809008479118347\n",
      "Epoch 1, Batch 13, Loss: -33.03559875488281\n",
      "Epoch 1, Batch 14, Loss: -4.036464214324951\n",
      "Epoch 1, Batch 15, Loss: -64.99867248535156\n",
      "Epoch 1, Batch 16, Loss: -74.98556518554688\n",
      "Epoch 1, Batch 17, Loss: -1.0018965005874634\n",
      "Epoch 1, Batch 18, Loss: 37.399658203125\n",
      "Epoch 2, Batch 1, Loss: -45.619754791259766\n",
      "Epoch 2, Batch 2, Loss: 77.39561462402344\n",
      "Epoch 2, Batch 3, Loss: -62.27387619018555\n",
      "Epoch 2, Batch 4, Loss: 35.35018539428711\n",
      "Epoch 2, Batch 5, Loss: -82.76883697509766\n",
      "Epoch 2, Batch 6, Loss: 76.07001495361328\n",
      "Epoch 2, Batch 7, Loss: -0.6601089239120483\n",
      "Epoch 2, Batch 8, Loss: -31.816652297973633\n",
      "Epoch 2, Batch 9, Loss: 4.626562118530273\n",
      "Epoch 2, Batch 10, Loss: -105.5614013671875\n",
      "Epoch 2, Batch 11, Loss: -105.62210083007812\n",
      "Epoch 2, Batch 12, Loss: -36.914207458496094\n",
      "Epoch 2, Batch 13, Loss: 0.0\n",
      "Epoch 2, Batch 14, Loss: -41.34808349609375\n",
      "Epoch 2, Batch 15, Loss: -30.691814422607422\n",
      "Epoch 2, Batch 16, Loss: -41.50586700439453\n",
      "Epoch 2, Batch 17, Loss: -64.89044189453125\n",
      "Epoch 2, Batch 18, Loss: -53.566165924072266\n",
      "Epoch 3, Batch 1, Loss: -86.43513488769531\n",
      "Epoch 3, Batch 2, Loss: 4.007994174957275\n",
      "Epoch 3, Batch 3, Loss: 52.5811882019043\n",
      "Epoch 3, Batch 4, Loss: -37.917442321777344\n",
      "Epoch 3, Batch 5, Loss: 13.216475486755371\n",
      "Epoch 3, Batch 6, Loss: -33.26667785644531\n",
      "Epoch 3, Batch 7, Loss: -75.16629791259766\n",
      "Epoch 3, Batch 8, Loss: -40.800968170166016\n",
      "Epoch 3, Batch 9, Loss: -39.38816452026367\n",
      "Epoch 3, Batch 10, Loss: -30.70045280456543\n",
      "Epoch 3, Batch 11, Loss: -3.3978843688964844\n",
      "Epoch 3, Batch 12, Loss: -63.22200012207031\n",
      "Epoch 3, Batch 13, Loss: -32.53913116455078\n",
      "Epoch 3, Batch 14, Loss: -44.78507995605469\n",
      "Epoch 3, Batch 15, Loss: -39.30446243286133\n",
      "Epoch 3, Batch 16, Loss: -46.9595832824707\n",
      "Epoch 3, Batch 17, Loss: -10.391048431396484\n",
      "Epoch 3, Batch 18, Loss: -47.298004150390625\n",
      "Epoch 4, Batch 1, Loss: -84.35279846191406\n",
      "Epoch 4, Batch 2, Loss: -33.91465377807617\n",
      "Epoch 4, Batch 3, Loss: -74.5223388671875\n",
      "Epoch 4, Batch 4, Loss: 2.7381358291833635e-10\n",
      "Epoch 4, Batch 5, Loss: 3.123563766479492\n",
      "Epoch 4, Batch 6, Loss: -35.41779327392578\n",
      "Epoch 4, Batch 7, Loss: 43.101531982421875\n",
      "Epoch 4, Batch 8, Loss: -72.7900619506836\n",
      "Epoch 4, Batch 9, Loss: -41.30385971069336\n",
      "Epoch 4, Batch 10, Loss: -48.03835678100586\n",
      "Epoch 4, Batch 11, Loss: -100.20126342773438\n",
      "Epoch 4, Batch 12, Loss: -20.799726486206055\n",
      "Epoch 4, Batch 13, Loss: 13.028127670288086\n",
      "Epoch 4, Batch 14, Loss: 5.4239630699157715\n",
      "Epoch 4, Batch 15, Loss: 37.810028076171875\n",
      "Epoch 4, Batch 16, Loss: -79.31346893310547\n",
      "Epoch 4, Batch 17, Loss: 19.858715057373047\n",
      "Epoch 4, Batch 18, Loss: -72.76395416259766\n",
      "Epoch 5, Batch 1, Loss: -22.154233932495117\n",
      "Epoch 5, Batch 2, Loss: 34.410980224609375\n",
      "Epoch 5, Batch 3, Loss: -83.09842681884766\n",
      "Epoch 5, Batch 4, Loss: -55.79152297973633\n",
      "Epoch 5, Batch 5, Loss: -41.83037567138672\n",
      "Epoch 5, Batch 6, Loss: -86.44356536865234\n",
      "Epoch 5, Batch 7, Loss: -1.5415363311767578\n",
      "Epoch 5, Batch 8, Loss: -3.9540977478027344\n",
      "Epoch 5, Batch 9, Loss: -78.36935424804688\n",
      "Epoch 5, Batch 10, Loss: 8.977434158325195\n",
      "Epoch 5, Batch 11, Loss: -54.98619842529297\n",
      "Epoch 5, Batch 12, Loss: -47.03689193725586\n",
      "Epoch 5, Batch 13, Loss: -44.78736877441406\n",
      "Epoch 5, Batch 14, Loss: -36.22397994995117\n",
      "Epoch 5, Batch 15, Loss: 46.47700119018555\n",
      "Epoch 5, Batch 16, Loss: -31.442352294921875\n",
      "Epoch 5, Batch 17, Loss: -83.13481903076172\n",
      "Epoch 5, Batch 18, Loss: -51.927677154541016\n"
     ]
    }
   ],
   "source": [
    "# After finding a suitable learning rate, update the optimizer\n",
    "optimizer = optim.Adam(lane_model.parameters(), lr=0.001)  # Update with the chosen learning rate\n",
    "\n",
    "# Train the model with the updated optimizer\n",
    "lane_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = lane_model(data)\n",
    "        target = target.view(-1, 1, 1, 1).expand_as(output).float()\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}, Batch {batch_idx+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycssQUQXA-ze"
   },
   "source": [
    "# Evaluate the trained model on the test set in evaluation mode, calculating the Mean Squared Error (MSE) and Mean Absolute Error (MAE) for each mini-batch, and then computes the average MSE and MAE over the entire test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "9jpWTTfvHh1U"
   },
   "outputs": [],
   "source": [
    "lane_model.eval()  # Set the model to evaluation mode\n",
    "total_mse = 0.0\n",
    "total_mae = 0.0\n",
    "num_samples = 0\n",
    "\n",
    "for data, target in test_loader:\n",
    "    with torch.no_grad():\n",
    "        output = lane_model(data)\n",
    "        target = target.view(-1, 1, 1, 1).expand_as(output).float()\n",
    "\n",
    "        # Calculate Mean Squared Error (MSE)\n",
    "        mse = nn.MSELoss()(output, target)\n",
    "        total_mse += mse.item()\n",
    "\n",
    "        # Calculate Mean Absolute Error (MAE)\n",
    "        mae = nn.L1Loss()(output, target)\n",
    "        total_mae += mae.item()\n",
    "\n",
    "        num_samples += data.size(0)\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_mse = total_mse / len(test_loader)\n",
    "avg_mae = total_mae / len(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PzBxEtLnjvko",
    "outputId": "7b496891-316f-44e5-a471-f974f110e41e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE: 44180.742513020836\n",
      "Average MAE: 163.7155532836914\n"
     ]
    }
   ],
   "source": [
    "print(\"Average MSE:\",avg_mse)\n",
    "print(\"Average MAE:\",avg_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yypOoC7GjSB7",
    "outputId": "2e4ebfb4-2ce8-4b41-f534-66f198253ba9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9444\n",
      "Accuracy: 0.8947\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "lane_model.eval()  # Set the model to evaluation mode\n",
    "threshold = 0.5\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for data, target in test_loader:\n",
    "    with torch.no_grad():\n",
    "        output = lane_model(data)\n",
    "        target = target.view(-1, 1, 1, 1).expand_as(output).float()\n",
    "\n",
    "        # Convert continuous predictions to binary\n",
    "        binary_predictions = (output > threshold).float()\n",
    "\n",
    "        true_labels.extend(target.cpu().numpy().flatten())\n",
    "        predicted_labels.extend(binary_predictions.cpu().numpy().flatten())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "true_labels = np.array(true_labels)\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "# Convert to binary labels based on the threshold\n",
    "binary_true_labels = (true_labels > threshold).astype(int)\n",
    "binary_predicted_labels = (predicted_labels > threshold).astype(int)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(binary_true_labels, binary_predicted_labels)\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(binary_true_labels, binary_predicted_labels)\n",
    "\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in c:\\users\\91767\\lokesh\\lib\\site-packages (4.7.1)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from gradio) (22.1.0)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from gradio) (5.2.0)\n",
      "Requirement already satisfied: fastapi in c:\\users\\91767\\lokesh\\lib\\site-packages (from gradio) (0.104.1)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\91767\\lokesh\\lib\\site-packages (from gradio) (0.3.1)\n",
      "Requirement already satisfied: gradio-client==0.7.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from gradio) (0.7.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\91767\\lokesh\\lib\\site-packages (from gradio) (0.25.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.14.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from gradio) (0.19.4)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in c:\\users\\91767\\lokesh\\lib\\site-packages (from gradio) (6.1.1)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from gradio) (2.1.1)\n",
      "Requirement already satisfied: matplotlib~=3.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from gradio) (3.7.1)\n",
      "Requirement already satisfied: numpy~=1.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from gradio) (1.24.3)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from gradio) (3.9.10)\n",
      "Requirement already satisfied: packaging in c:\\users\\91767\\lokesh\\lib\\site-packages (from gradio) (23.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from gradio) (1.5.3)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from gradio) (9.4.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from gradio) (2.5.2)\n",
      "Requirement already satisfied: pydub in c:\\users\\91767\\lokesh\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in c:\\users\\91767\\lokesh\\lib\\site-packages (from gradio) (0.0.6)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from gradio) (6.0)\n",
      "Requirement already satisfied: requests~=2.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from gradio) (2.31.0)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.9 in c:\\users\\91767\\lokesh\\lib\\site-packages (from typer[all]<1.0,>=0.9->gradio) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from gradio) (4.8.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from gradio) (0.24.0.post1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\91767\\lokesh\\lib\\site-packages (from gradio-client==0.7.0->gradio) (2023.10.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from gradio-client==0.7.0->gradio) (11.0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from altair<6.0,>=4.2.0->gradio) (4.17.3)\n",
      "Requirement already satisfied: toolz in c:\\users\\91767\\lokesh\\lib\\site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\91767\\lokesh\\lib\\site-packages (from huggingface-hub>=0.14.0->gradio) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\91767\\lokesh\\lib\\site-packages (from huggingface-hub>=0.14.0->gradio) (4.65.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\91767\\lokesh\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\91767\\lokesh\\lib\\site-packages (from matplotlib~=3.0->gradio) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from matplotlib~=3.0->gradio) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\91767\\lokesh\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\91767\\lokesh\\lib\\site-packages (from matplotlib~=3.0->gradio) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\91767\\lokesh\\lib\\site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\91767\\lokesh\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2022.7)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\91767\\lokesh\\lib\\site-packages (from pydantic>=2.0->gradio) (2.14.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\91767\\lokesh\\lib\\site-packages (from requests~=2.0->gradio) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91767\\lokesh\\lib\\site-packages (from requests~=2.0->gradio) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\91767\\lokesh\\lib\\site-packages (from requests~=2.0->gradio) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91767\\lokesh\\lib\\site-packages (from requests~=2.0->gradio) (2023.7.22)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\91767\\lokesh\\lib\\site-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in c:\\users\\91767\\lokesh\\lib\\site-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.0)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\91767\\lokesh\\lib\\site-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in c:\\users\\91767\\lokesh\\lib\\site-packages (from fastapi->gradio) (3.7.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from fastapi->gradio) (0.27.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\91767\\lokesh\\lib\\site-packages (from httpx->gradio) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\91767\\lokesh\\lib\\site-packages (from httpx->gradio) (1.2.0)\n",
      "Requirement already satisfied: six in c:\\users\\91767\\lokesh\\lib\\site-packages (from cycler>=0.10->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (22.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\91767\\lokesh\\lib\\site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\91767\\lokesh\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.0)\n",
      "Python 3.11.4\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened Size: 2048\n"
     ]
    }
   ],
   "source": [
    "class MiniLaneDetectionModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(MiniLaneDetectionModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Add more layers as needed\n",
    "        )\n",
    "        # Print the shape of the flattened output before the linear layer\n",
    "        dummy_input = torch.randn(2, 3, 32, 32)\n",
    "        features_output = self.features(dummy_input)\n",
    "        flattened_size = features_output.view(features_output.size(0), -1).shape[1]\n",
    "        print(\"Flattened Size:\", flattened_size)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(flattened_size, 2048),  # Adjust this line\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(2048, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Check the size of the output after the convolutional layers\n",
    "mini_lane_model = MiniLaneDetectionModel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std(loader):\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for data, _ in loader:\n",
    "        batch_samples = data.size(0)\n",
    "        data = data.view(batch_samples, data.size(1), -1)\n",
    "        mean += data.mean(2).sum(0)\n",
    "        std += data.std(2).sum(0)\n",
    "        total_samples += batch_samples\n",
    "\n",
    "    mean /= total_samples\n",
    "    std /= total_samples\n",
    "\n",
    "    return mean.tolist(), std.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, temp_dataset = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "val_dataset, test_dataset = torch.utils.data.random_split(temp_dataset, [val_size, test_size])\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean, train_std = calculate_mean_std(train_loader)\n",
    "# Define transforms for normalization and augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=train_mean, std=train_std),\n",
    "])\n",
    "\n",
    "# Load dataset with the defined transforms\n",
    "dataset = CustomDataset(root_dir=\"/Downloads\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data Shape: torch.Size([2, 3, 32, 32])\n",
      "Convolutional Output Shape: torch.Size([2, 32, 8, 8])\n",
      "Flattened Output Shape: torch.Size([2, 2048])\n",
      "Epoch 1, Loss: 0.6477693319320679\n",
      "Input Data Shape: torch.Size([2, 3, 32, 32])\n",
      "Convolutional Output Shape: torch.Size([2, 32, 8, 8])\n",
      "Flattened Output Shape: torch.Size([2, 2048])\n",
      "Epoch 2, Loss: 0.629828155040741\n",
      "Input Data Shape: torch.Size([2, 3, 32, 32])\n",
      "Convolutional Output Shape: torch.Size([2, 32, 8, 8])\n",
      "Flattened Output Shape: torch.Size([2, 2048])\n",
      "Epoch 3, Loss: 0.6249784231185913\n",
      "Input Data Shape: torch.Size([2, 3, 32, 32])\n",
      "Convolutional Output Shape: torch.Size([2, 32, 8, 8])\n",
      "Flattened Output Shape: torch.Size([2, 2048])\n",
      "Epoch 4, Loss: 0.5843857526779175\n",
      "Input Data Shape: torch.Size([2, 3, 32, 32])\n",
      "Convolutional Output Shape: torch.Size([2, 32, 8, 8])\n",
      "Flattened Output Shape: torch.Size([2, 2048])\n",
      "Epoch 5, Loss: 0.539573073387146\n",
      "Input Data Shape: torch.Size([2, 3, 32, 32])\n",
      "Convolutional Output Shape: torch.Size([2, 32, 8, 8])\n",
      "Flattened Output Shape: torch.Size([2, 2048])\n",
      "Epoch 6, Loss: 0.5203258991241455\n",
      "Input Data Shape: torch.Size([2, 3, 32, 32])\n",
      "Convolutional Output Shape: torch.Size([2, 32, 8, 8])\n",
      "Flattened Output Shape: torch.Size([2, 2048])\n",
      "Epoch 7, Loss: 0.46612006425857544\n",
      "Input Data Shape: torch.Size([2, 3, 32, 32])\n",
      "Convolutional Output Shape: torch.Size([2, 32, 8, 8])\n",
      "Flattened Output Shape: torch.Size([2, 2048])\n",
      "Epoch 8, Loss: 0.41396573185920715\n",
      "Input Data Shape: torch.Size([2, 3, 32, 32])\n",
      "Convolutional Output Shape: torch.Size([2, 32, 8, 8])\n",
      "Flattened Output Shape: torch.Size([2, 2048])\n",
      "Epoch 9, Loss: 0.40396684408187866\n",
      "Input Data Shape: torch.Size([2, 3, 32, 32])\n",
      "Convolutional Output Shape: torch.Size([2, 32, 8, 8])\n",
      "Flattened Output Shape: torch.Size([2, 2048])\n",
      "Epoch 10, Loss: 0.3477143943309784\n",
      "Input Data Shape: torch.Size([2, 3, 32, 32])\n",
      "Convolutional Output Shape: torch.Size([2, 32, 8, 8])\n",
      "Flattened Output Shape: torch.Size([2, 2048])\n",
      "Epoch 11, Loss: 0.3068261742591858\n",
      "Input Data Shape: torch.Size([2, 3, 32, 32])\n",
      "Convolutional Output Shape: torch.Size([2, 32, 8, 8])\n",
      "Flattened Output Shape: torch.Size([2, 2048])\n",
      "Epoch 12, Loss: 0.3028492331504822\n",
      "Input Data Shape: torch.Size([2, 3, 32, 32])\n",
      "Convolutional Output Shape: torch.Size([2, 32, 8, 8])\n",
      "Flattened Output Shape: torch.Size([2, 2048])\n",
      "Epoch 13, Loss: 0.2459351122379303\n",
      "Input Data Shape: torch.Size([2, 3, 32, 32])\n",
      "Convolutional Output Shape: torch.Size([2, 32, 8, 8])\n",
      "Flattened Output Shape: torch.Size([2, 2048])\n",
      "Epoch 14, Loss: 0.22330164909362793\n",
      "Input Data Shape: torch.Size([2, 3, 32, 32])\n",
      "Convolutional Output Shape: torch.Size([2, 32, 8, 8])\n",
      "Flattened Output Shape: torch.Size([2, 2048])\n",
      "Epoch 15, Loss: 0.17741435766220093\n",
      "Input Data Shape: torch.Size([2, 3, 32, 32])\n",
      "Convolutional Output Shape: torch.Size([2, 32, 8, 8])\n",
      "Flattened Output Shape: torch.Size([2, 2048])\n",
      "Epoch 16, Loss: 0.18673458695411682\n",
      "Input Data Shape: torch.Size([2, 3, 32, 32])\n",
      "Convolutional Output Shape: torch.Size([2, 32, 8, 8])\n",
      "Flattened Output Shape: torch.Size([2, 2048])\n",
      "Epoch 17, Loss: 0.1283397376537323\n",
      "Input Data Shape: torch.Size([2, 3, 32, 32])\n",
      "Convolutional Output Shape: torch.Size([2, 32, 8, 8])\n",
      "Flattened Output Shape: torch.Size([2, 2048])\n",
      "Epoch 18, Loss: 0.11351335048675537\n",
      "Input Data Shape: torch.Size([2, 3, 32, 32])\n",
      "Convolutional Output Shape: torch.Size([2, 32, 8, 8])\n",
      "Flattened Output Shape: torch.Size([2, 2048])\n",
      "Epoch 19, Loss: 0.08776289224624634\n",
      "Input Data Shape: torch.Size([2, 3, 32, 32])\n",
      "Convolutional Output Shape: torch.Size([2, 32, 8, 8])\n",
      "Flattened Output Shape: torch.Size([2, 2048])\n",
      "Epoch 20, Loss: 0.08283960819244385\n"
     ]
    }
   ],
   "source": [
    "# (i) Overfitting with 100% accuracy (train on a small subset)\n",
    "num_classes = 2\n",
    "overfit_dataset = torch.utils.data.Subset(train_dataset, range(2))\n",
    "overfit_loader = DataLoader(overfit_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Train the mini model on the small subset for overfitting\n",
    "# Train the mini model on the small subset for overfitting\n",
    "mini_lane_model.train()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(mini_lane_model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(20):  # Train for a small number of epochs for overfitting\n",
    "    for batch_idx, (data, target) in enumerate(overfit_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Debugging: Print the shape of the input data\n",
    "        print(\"Input Data Shape:\", data.shape)\n",
    "\n",
    "        # Debugging: Print the shape of the output after the convolutional layers\n",
    "        conv_output = mini_lane_model.features(data)\n",
    "        print(\"Convolutional Output Shape:\", conv_output.shape)\n",
    "\n",
    "        # Debugging: Print the shape of the flattened output before the linear layer\n",
    "        flattened_output = conv_output.view(conv_output.size(0), -1)\n",
    "        print(\"Flattened Output Shape:\", flattened_output.shape)\n",
    "\n",
    "        # Ensure that target values are within the correct range (e.g., 0 and 1 for binary classification)\n",
    "        # Update the target tensor accordingly\n",
    "        target = target % num_classes  # Adjust this line based on your number of classes\n",
    "\n",
    "        output = mini_lane_model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()b\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs a training loop for the given number of epochs, with each epoch iterating over batches in the training DataLoader (train_loader). After each epoch, the model is evaluated on the validation DataLoader (val_loader). The training and validation losses are stored for later plotting. The final step involves plotting the training and validation losses over epochs. Adjustments may be needed based on the specific architecture and requirements of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened Size: 2048\n",
      "Epoch 1, Training Loss: 0.7199132442474365, Validation Loss: 0.6893423199653625\n",
      "Epoch 2, Training Loss: 0.7395115494728088, Validation Loss: 0.6985534429550171\n",
      "Epoch 3, Training Loss: 0.7114754319190979, Validation Loss: 0.6887160142262777\n",
      "Epoch 4, Training Loss: 0.7294949889183044, Validation Loss: 0.6936815977096558\n",
      "Epoch 5, Training Loss: 0.735637903213501, Validation Loss: 0.7024547457695007\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training loop\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "model=MiniLaneDetectionModel()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Ensure that target values are within the correct range (e.g., 0 and 1 for binary classification)\n",
    "        target = target % num_classes  # Adjust this line based on your number of classes\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        validation_loss = 0.0\n",
    "        for data, target in val_loader:\n",
    "            # Ensure that target values are within the correct range for validation\n",
    "            target = target % num_classes  # Adjust this line based on your number of classes\n",
    "\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        validation_losses.append(validation_loss / len(val_loader))\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Training Loss: {loss.item()}, Validation Loss: {validation_loss / len(val_loader)}\")\n",
    "\n",
    "# Plotting\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(validation_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code evaluates the model on the test set, calculating the total loss and accuracy. Make sure to adjust the target range transformation based on your specific number of classes. The results are then printed or logged for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on Test Set - Accuracy: 0.6842\n",
      "Total Loss on Test Set: 2.0240\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the device\n",
    "model.to(device)\n",
    "# Test the model on the test set\n",
    "model.eval()\n",
    "\n",
    "# Variables to store predictions and ground truth\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "# Variable to store total loss for the entire test set\n",
    "total_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        # Ensure that target values are within the correct range for testing\n",
    "        target = target % num_classes  # Adjust this line based on your number of classes\n",
    "\n",
    "        # Move data and target to the device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Accumulate total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Store predictions and targets\n",
    "        all_predictions.append(output.argmax(dim=1))  # Assuming a classification model\n",
    "        all_targets.append(target)\n",
    "\n",
    "# Concatenate predictions and targets along the batch dimension\n",
    "all_predictions = torch.cat(all_predictions, dim=0)\n",
    "all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "# Calculate performance metrics (e.g., accuracy)\n",
    "correct_predictions = (all_predictions == all_targets).sum().item()\n",
    "total_samples = all_targets.numel()\n",
    "accuracy = correct_predictions / total_samples\n",
    "\n",
    "# Print or log the results\n",
    "print(f\"Performance on Test Set - Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Total Loss on Test Set: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: lane_detection_model.pth\n"
     ]
    }
   ],
   "source": [
    "save_path = \"lane_detection_model.pth\"  # You can customize the path and filename\n",
    "torch.save(model.state_dict(), save_path)\n",
    "\n",
    "print(f\"Model saved at: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a Gradio interface where users can input an image, and the model will segment the lanes and provide the result as an output image. Users can interact with this interface to see real-time results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZAkTXfeANgkM",
    "outputId": "0ff902e0-adbe-4827-b29e-56c5f8ca2392",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened Size: 2048\n",
      "Running on local URL:  http://127.0.0.1:7872\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7872/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gradio as gr\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Define the path to your saved model checkpoint\n",
    "checkpoint_path = 'lane_detection_model.pth'\n",
    "\n",
    "# Load the saved model\n",
    "model = MiniLaneDetectionModel()\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "model.eval()\n",
    "\n",
    "# Define the transform for preprocessing the input image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Define the Gradio function for segmentation\n",
    "import torch\n",
    "import gradio as gr\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# ... (same as before)\n",
    "\n",
    "# Define the Gradio function for segmentation\n",
    "def segment_lanes(image):\n",
    "    # Apply the transform to the input image\n",
    "    input_image = Image.fromarray(image.astype('uint8'), 'RGB')\n",
    "    input_tensor = transform(input_image)\n",
    "    input_batch = input_tensor.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "    # Forward pass to obtain the segmentation mask\n",
    "    with torch.no_grad():\n",
    "        segmentation_mask = model(input_batch)\n",
    "\n",
    "    # Assuming your model produces a tensor representing the segmentation mask,\n",
    "    # you may want to apply a threshold to convert it into a binary mask (lane or not lane)\n",
    "    threshold = 0.5  # Adjust this threshold as needed\n",
    "    binary_mask = (segmentation_mask > threshold).float()\n",
    "\n",
    "    # Convert PyTorch tensor to NumPy array for compatibility\n",
    "    binary_mask_np = binary_mask.squeeze().cpu().numpy()\n",
    "\n",
    "    # Ensure that binary_mask_np is 2D (not 3D)\n",
    "    binary_mask_np = binary_mask_np[0] if len(binary_mask_np.shape) == 3 else binary_mask_np\n",
    "\n",
    "    # Resize the binary_mask_np to match the shape of the input image\n",
    "    binary_mask_np = np.resize(binary_mask_np, image.shape[:2])\n",
    "\n",
    "    # Create a colored overlay using the original image and the binary mask\n",
    "    segmented_image = np.zeros_like(image, dtype=np.uint8)\n",
    "    segmented_image[:, :, 0] = (image[:, :, 0] * (1 - binary_mask_np)).astype(np.uint8)  # Red channel\n",
    "    segmented_image[:, :, 1] = (image[:, :, 1] * (1 - binary_mask_np)).astype(np.uint8)  # Green channel\n",
    "    segmented_image[:, :, 2] = (image[:, :, 2] + 255 * binary_mask_np).astype(np.uint8)  # Blue channel\n",
    "\n",
    "    return segmented_image\n",
    "\n",
    "# Create the Gradio interface\n",
    "iface = gr.Interface(fn=segment_lanes, inputs=\"image\", outputs=\"image\")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
